#!/usr/bin/env bash
set -e

cd "$(dirname "$0")"

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
CYAN='\033[0;36m'
NC='\033[0m'

log_info()  { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn()  { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

check_nix() {
    if [[ -z "${IN_NIX_SHELL:-}" ]]; then
        log_error "Not in nix shell. Run 'nix develop' first."
        exit 1
    fi
}

cmd_start() {
    check_nix
    local backend="${1:-}"
    local force="${2:-}"

    if [[ -z "$backend" ]]; then
        log_error "Usage: ./toolbox start <backend>"
        echo ""
        echo "Available backends:"
        python -c "
from manager import get_all_backends
for name, b in get_all_backends().items():
    reason = b.get_unavailable_reason()
    status = f' ({reason})' if reason else ''
    print(f'  {name:15} {b.display_name}{status}')
"
        exit 1
    fi

    python -c "
from manager import start_backend
success, msg = start_backend('$backend', force=True)
print(msg)
exit(0 if success else 1)
"
}

cmd_stop() {
    check_nix
    local backend="${1:-}"

    if [[ -z "$backend" ]]; then
        # Stop whatever is running
        python -c "
from manager import status, stop_backend, get_active_backend

active = get_active_backend()
if not active:
    print('No backend running')
else:
    if stop_backend(active):
        print(f'{active.display_name} stopped')
    else:
        print(f'Failed to stop {active.display_name}')
        exit(1)
"
    else
        # Stop specific backend - just call stop(), trust the result
        # Don't check is_healthy() first - that can trigger expensive SDK init
        python -c "
from manager import get_backend, stop_backend

backend = get_backend('$backend')
if not backend:
    print(f'Unknown backend: $backend')
    exit(1)

if stop_backend(backend):
    print(f'{backend.display_name} stopped')
else:
    print(f'Failed to stop {backend.display_name}')
    exit(1)
"
    fi
}

cmd_status() {
    check_nix
    python -c "
from manager import status, get_all_backends

s = status()
backends = get_all_backends()

if s.conflict:
    print('WARNING: Multiple backends running!')
    for name, healthy in s.backends.items():
        if healthy:
            print(f'  - {backends[name].display_name} (running)')
elif s.active:
    print(f'Active: {s.active.display_name}')
    print(f'API:    {s.active.api_base}')

    # Show loaded model if any
    loaded = s.active.get_loaded_model()
    if loaded:
        print(f'Model:  {loaded.name}')
else:
    print('No backend running')

print('')
print('Backends:')
for name, healthy in s.backends.items():
    b = backends[name]
    if healthy:
        status_str = '●'
        suffix = ''
    else:
        status_str = '○'
        reason = b.get_unavailable_reason()
        suffix = f' ({reason})' if reason else ''
    print(f'  {status_str} {b.display_name}{suffix}')
"
}

cmd_models() {
    check_nix
    local backend="${1:-}"

    python -c "
from manager import list_models, get_active_backend, get_all_backends

backend_name = '$backend' or None
if not backend_name:
    active = get_active_backend()
    if active:
        backend_name = active.name
    else:
        print('No backend active. Specify a backend: ./toolbox models <backend>')
        print('')
        print('Available backends:')
        from manager.registry import get_registry
        registry = get_registry()
        for name in registry.get_available().keys():
            print(f'  {name}')
        exit(1)

models = list_models(backend_name)
if not models or not models.get(backend_name):
    print(f'No models found for {backend_name}')
    print('')
    print('Add GGUF models to ~/.local/share/models/')
    exit(0)

print(f'Models for {backend_name}:')
for m in models[backend_name]:
    size_mb = m.size_bytes / (1024 * 1024) if m.size_bytes else 0
    size_str = f'{size_mb/1024:.1f} GB' if size_mb >= 1024 else f'{size_mb:.0f} MB'
    quant = f' [{m.quantization}]' if m.quantization else ''
    print(f'  {m.id}{quant} ({size_str})')
"
}

cmd_load() {
    check_nix
    local model="${1:-}"
    local backend="${2:-}"

    if [[ -z "$model" ]]; then
        log_error "Usage: ./toolbox load <model> [backend]"
        echo ""
        echo "Load a model on the active (or specified) backend."
        echo "List models with: ./toolbox models"
        exit 1
    fi

    python -c "
from manager import load_model, get_active_backend

backend_name = '$backend' or None
if not backend_name:
    active = get_active_backend()
    if active:
        backend_name = active.name
    else:
        print('No backend active. Start one first: ./toolbox start <backend>')
        exit(1)

success, msg = load_model(backend_name, '$model')
print(msg)
exit(0 if success else 1)
"
}

cmd_unload() {
    check_nix
    local backend="${1:-}"

    python -c "
from manager import unload_model, get_active_backend

backend_name = '$backend' or None
if not backend_name:
    active = get_active_backend()
    if active:
        backend_name = active.name
    else:
        print('No backend active')
        exit(0)

success, msg = unload_model(backend_name)
print(msg)
exit(0 if success else 1)
"
}

cmd_download() {
    local model="$1"
    local backend="$2"

    if [[ -z "$model" ]]; then
        log_error "Usage: ./toolbox download <model> [backend]"
        echo ""
        echo "Download a model for a backend."
        echo ""
        echo "Arguments:"
        echo "  model     Model ID to download (e.g., 'phi-4' for Foundry, HuggingFace repo for llama)"
        echo "  backend   Backend name (default: active backend)"
        echo ""
        echo "Examples:"
        echo "  ./toolbox download phi-4                    # Download for active backend"
        echo "  ./toolbox download phi-4 foundry            # Download for Foundry"
        echo "  ./toolbox download bartowski/Qwen2.5-1.5B-Instruct-GGUF llama"
        exit 1
    fi

    check_nix

    python -c "
from manager import download_model, get_active_backend, get_backend

model_id = '$model'
backend_name = '$backend' or None

if not backend_name:
    active = get_active_backend()
    if active:
        backend_name = active.name
    else:
        print('No backend active. Specify a backend: ./toolbox download <model> <backend>')
        exit(1)

from manager.registry import get_registry
registry = get_registry()

backend = get_backend(backend_name)
if not backend:
    print(f'Unknown backend: {backend_name}')
    exit(1)

if not registry.is_available(backend_name):
    print(f'{backend.display_name}: {registry.get_unavailable_reason(backend_name)}')
    exit(1)

print(f'Downloading {model_id} for {backend.display_name}...')
success, msg = download_model(backend_name, model_id)
print(msg)
exit(0 if success else 1)
"
}

cmd_serve() {
    check_nix
    local port="${PORT:-8090}"
    local debug_flag=""

    # Check for --debug or -d flag
    for arg in "$@"; do
        if [[ "$arg" == "--debug" || "$arg" == "-d" ]]; then
            debug_flag="--debug"
            log_info "Debug logging enabled"
        fi
    done

    log_info "Starting Local LLM Toolbox..."
    echo ""
    echo -e "  Dashboard: ${CYAN}http://localhost:${port}${NC} (Web UI)"
    echo -e "  API:       ${CYAN}http://localhost:${port}/v1${NC} (OpenAI-compatible)"
    echo ""
    log_info "Press Ctrl+C to stop"
    echo ""

    # Cleanup on exit - called when python exits or on signal
    cleanup() {
        trap - EXIT INT TERM HUP  # Prevent re-entry
        echo ""
        log_info "Stopping backends..."
        # Stop backends via manager
        python -c "from manager import stop_all; stop_all()" 2>/dev/null || true
        # Kill any orphaned llama-server processes
        pkill -15 llama-server 2>/dev/null || true
        log_info "Stopped"
    }
    trap cleanup EXIT

    # Run the unified app
    python -m dashboard $debug_flag
}

cmd_rpc() {
    check_nix
    local backend="${1:-}"
    shift || true

    if [[ -z "$backend" ]]; then
        log_error "Usage: ./toolbox rpc <backend> [options]"
        echo ""
        echo "Start an RPC worker for distributed inference."
        echo ""
        echo "Supported backends:"
        echo "  llama     llama.cpp RPC worker"
        echo ""
        echo "Options for 'llama':"
        echo "  --rpc-port <port>      RPC server port (default: 50052)"
        echo "  --control-port <port>  Control API port (default: 50053)"
        echo ""
        echo "Example:"
        echo "  ./toolbox rpc llama"
        echo "  ./toolbox rpc llama --rpc-port 50052 --control-port 50053"
        exit 1
    fi

    case "$backend" in
        llama)
            cmd_rpc_llama "$@"
            ;;
        *)
            log_error "Unknown backend for RPC: $backend"
            echo "Supported: llama"
            exit 1
            ;;
    esac
}

cmd_rpc_llama() {
    local rpc_port="${LLAMA_RPC_PORT:-50052}"
    local control_port="${LLAMA_RPC_CONTROL_PORT:-50053}"

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --rpc-port)
                rpc_port="$2"
                shift 2
                ;;
            --control-port)
                control_port="$2"
                shift 2
                ;;
            *)
                log_error "Unknown option: $1"
                exit 1
                ;;
        esac
    done

    log_info "Starting llama.cpp RPC worker..."
    echo ""
    echo -e "  RPC server:   ${CYAN}0.0.0.0:${rpc_port}${NC} (tensor offload)"
    echo -e "  Control API:  ${CYAN}0.0.0.0:${control_port}${NC} (management)"
    echo ""
    log_info "Press Ctrl+C to stop"
    echo ""

    python -c "
from manager.rpc import RpcWorkerServer

server = RpcWorkerServer(
    rpc_port=$rpc_port,
    control_port=$control_port,
)
server.run()
"
}

cmd_help() {
    echo -e "${CYAN}Local LLM Toolbox${NC}"
    echo ""
    echo "Usage: ./toolbox <command> [args]"
    echo ""
    echo "Commands:"
    echo "  serve               Start the dashboard + API server"
    echo "  start <backend>     Start a backend (llama, foundry)"
    echo "  stop [backend]      Stop the active backend"
    echo "  status              Show current status"
    echo "  models [backend]    List available models"
    echo "  load <model>        Load a model"
    echo "  unload              Unload current model"
    echo "  download <model>    Download a model"
    echo "  rpc <backend>       Start as RPC worker (for distributed inference)"
    echo ""
    echo "Examples:"
    echo "  ./toolbox serve                    # Start dashboard at :8090"
    echo "  ./toolbox start llama && ./toolbox load my-model"
    echo ""
    echo "See './toolbox rpc' for distributed inference setup."
}

case "${1:-help}" in
    start)     cmd_start "$2" "$3" ;;
    stop)      cmd_stop "$2" ;;
    status)    cmd_status ;;
    models)    cmd_models "$2" ;;
    download)  cmd_download "$2" "$3" ;;
    load)      cmd_load "$2" "$3" ;;
    unload)    cmd_unload "$2" ;;
    serve)     shift; cmd_serve "$@" ;;
    rpc)       shift; cmd_rpc "$@" ;;
    help|-h)   cmd_help ;;
    *)
        log_error "Unknown command: $1"
        cmd_help
        exit 1
        ;;
esac
