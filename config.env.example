# =============================================================================
# Local LLM Toolbox - Configuration
# =============================================================================
# Copy to .env and adjust values as needed.
# All values shown are the defaults.

# -----------------------------------------------------------------------------
# Router (API proxy)
# -----------------------------------------------------------------------------
ROUTER_HOST=0.0.0.0
ROUTER_PORT=5001  # 5000 is often used by AirPlay on macOS
ROUTER_HEALTH_INTERVAL=5

# -----------------------------------------------------------------------------
# Dashboard (Web UI)
# -----------------------------------------------------------------------------
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8090

# -----------------------------------------------------------------------------
# llama.cpp Backend
# -----------------------------------------------------------------------------
# Model storage directory
LLAMA_MODELS_DIR=~/.local/share/models

# Server settings
LLAMA_HOST=0.0.0.0
LLAMA_PORT=8080
LLAMA_CTX_SIZE=8192
LLAMA_GPU_LAYERS=all             # 'all', 'auto', or number of layers

# Timeouts (seconds)
LLAMA_LOAD_TIMEOUT=1800          # 30 min - time to wait for model to load
LLAMA_DOWNLOAD_TIMEOUT=14400     # 4 hours - for large model downloads
LLAMA_GRACEFUL_TIMEOUT=15        # graceful shutdown timeout

# RPC Cluster (optional - for distributed inference)
# Workers run: ./toolbox rpc llama
# Then configure workers here on the main node:
# LLAMA_RPC_WORKERS=192.168.1.10,192.168.1.11   # Comma-separated worker IPs
# LLAMA_RPC_CONTROL_PORT=50053                   # Control API port (default: 50053)

# -----------------------------------------------------------------------------
# Foundry Local Backend (macOS/Windows only)
# -----------------------------------------------------------------------------
FOUNDRY_PORT=5273
FOUNDRY_LOAD_TIMEOUT=1800
FOUNDRY_DOWNLOAD_TIMEOUT=14400

# -----------------------------------------------------------------------------
# vLLM Backend (Linux + CUDA only)
# -----------------------------------------------------------------------------
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_LOAD_TIMEOUT=1800
VLLM_DOWNLOAD_TIMEOUT=14400

# -----------------------------------------------------------------------------
# SGLang Backend (Linux + CUDA only)
# -----------------------------------------------------------------------------
SGLANG_HOST=0.0.0.0
SGLANG_PORT=30000
SGLANG_LOAD_TIMEOUT=1800
SGLANG_DOWNLOAD_TIMEOUT=14400
