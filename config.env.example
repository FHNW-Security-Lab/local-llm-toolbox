# =============================================================================
# Local LLM Toolbox - Configuration
# =============================================================================
# Copy to .env and adjust values as needed.
# All values shown are the defaults.

# -----------------------------------------------------------------------------
# Router (API proxy)
# -----------------------------------------------------------------------------
ROUTER_HOST=0.0.0.0
ROUTER_PORT=5001  # 5000 is often used by AirPlay on macOS
ROUTER_HEALTH_INTERVAL=5

# -----------------------------------------------------------------------------
# Dashboard (Web UI)
# -----------------------------------------------------------------------------
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8090

# -----------------------------------------------------------------------------
# llama.cpp Backend
# -----------------------------------------------------------------------------
# Model storage directory
LLAMA_MODELS_DIR=~/.local/share/models

# Server settings
LLAMA_HOST=0.0.0.0
LLAMA_PORT=8080
LLAMA_CTX_SIZE=8192
LLAMA_GPU_LAYERS=99

# Timeouts (seconds)
LLAMA_LOAD_TIMEOUT=1800          # 30 min - time to wait for model to load
LLAMA_DOWNLOAD_TIMEOUT=14400     # 4 hours - for large model downloads
LLAMA_GRACEFUL_TIMEOUT=15        # graceful shutdown timeout

# RPC Cluster (optional - for distributed inference)
# LLAMA_RPC_HOST=192.168.1.100   # Remote node hostname/IP
# LLAMA_RPC_PORT=50052           # RPC server port on remote
# LLAMA_SSH_USER=llm             # SSH username for remote
# LLAMA_SSH_PORT=22              # SSH port
# LLAMA_SSH_TIMEOUT=30           # SSH connection timeout

# -----------------------------------------------------------------------------
# Foundry Local Backend (macOS/Windows only)
# -----------------------------------------------------------------------------
FOUNDRY_PORT=5273
FOUNDRY_LOAD_TIMEOUT=1800
FOUNDRY_DOWNLOAD_TIMEOUT=14400

# -----------------------------------------------------------------------------
# vLLM Backend (Linux + CUDA only)
# -----------------------------------------------------------------------------
VLLM_HOST=0.0.0.0
VLLM_PORT=8000
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.9
VLLM_LOAD_TIMEOUT=1800
VLLM_DOWNLOAD_TIMEOUT=14400

# -----------------------------------------------------------------------------
# SGLang Backend (Linux + CUDA only)
# -----------------------------------------------------------------------------
SGLANG_HOST=0.0.0.0
SGLANG_PORT=30000
SGLANG_LOAD_TIMEOUT=1800
SGLANG_DOWNLOAD_TIMEOUT=14400
